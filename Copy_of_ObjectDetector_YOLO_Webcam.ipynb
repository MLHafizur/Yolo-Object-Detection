{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ObjectDetector_YOLO_Webcam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qRaZzSDKZe1"
      },
      "source": [
        "from imutils.video import VideoStream\n",
        "from imutils.video import FPS\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import imutils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX9Voz5VKaHi"
      },
      "source": [
        "yoloPath=\"/content/Obj_DetecTion\"\n",
        "#yolo --base path to YOLO directory\n",
        "#confidence --minimum probability to filter weak detections\n",
        "#threshold --threshold when applyong non-maxima suppression\n",
        "args={'yolo': yoloPath , 'confidence': 0.5, 'threshold': 0.3}"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0MSlvBYKfPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71af7ed3-6054-4cfd-cf04-b054e014295c"
      },
      "source": [
        "# load the COCO class labels our YOLO model was trained on\n",
        "LABELS = open(yoloPath+\"//coco.names\").read().strip().split(\"\\n\")\n",
        "print(\"Toal classes {0}\".format(len(LABELS)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Toal classes 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjfwpEsIKjsD"
      },
      "source": [
        "# initialize a list of colors to represent each possible class label\n",
        "np.random.seed(42)\n",
        "#create random list of int type numbers from range 0-255. Size = len(LABELS), 3... 3 is for RGB\n",
        "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),dtype=\"uint8\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdEW-O_kKkq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762bfbaf-d6a1-4bd6-a9d6-35a134494fc9"
      },
      "source": [
        "!pip install --upgrade opencv-python==4.4.0.40\n",
        "# derive the paths to the YOLO weights and model configuration\n",
        "weightsPath = \"/content/Obj_DetecTion/yolov4-obj_5000.weights\"\n",
        "configPath = yoloPath+\"//yolov4-obj.cfg\"\n",
        "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
        "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: opencv-python==4.4.0.40 in /usr/local/lib/python3.7/dist-packages (4.4.0.40)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.4.0.40) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u301D1WbKmu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78bf1b0f-2d61-4d29-bfd8-cd95cc4bcf1e"
      },
      "source": [
        "#Generally in a sequential CNN network there will be only one output layer at the end. \n",
        "#In the YOLO v3 architecture we are using there are multiple output layers giving out predictions.\n",
        "ln_all = net.getLayerNames()\n",
        "#print(ln)\n",
        "\n",
        "ln=[]\n",
        "# determine only the *output* layer names that we need from YOLO\n",
        "for i in ln_all:\n",
        "    if \"yolo\" in i:\n",
        "        ln.append(i)\n",
        "        \n",
        "print(ln)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['yolo_139', 'yolo_150', 'yolo_161']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9DHf_wqp6or"
      },
      "source": [
        "'''\n",
        "## Camera Capture\n",
        "Using a webcam to capture images for processing on the runtime.\n",
        "Source: https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=2viqYx97hPMi\n",
        "'''\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZUsGg7RmJeN"
      },
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\r\n",
        "def js_to_image(js_reply):\r\n",
        "  \"\"\"\r\n",
        "  Params:\r\n",
        "          js_reply: JavaScript object containing image from webcam\r\n",
        "  Returns:\r\n",
        "          img: OpenCV BGR image\r\n",
        "  \"\"\"\r\n",
        "  # decode base64 image\r\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\r\n",
        "  # convert bytes to numpy array\r\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\r\n",
        "  # decode numpy array into OpenCV BGR image\r\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\r\n",
        "\r\n",
        "  return img"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqDOXMiRrJ6l"
      },
      "source": [
        "with open(r'/content/Obj_DetecTion/coco.names') as f:\r\n",
        "    # Getting labels reading every line\r\n",
        "    # and putting them into the list\r\n",
        "    labels = [line.strip() for line in f]\r\n",
        "\r\n",
        "network = cv2.dnn.readNetFromDarknet(r'/content/Obj_DetecTion/yolovhafiz.cfg',\r\n",
        "                                     r'/content/Obj_DetecTion/yolov4-obj_5000.weights')\r\n",
        "\r\n",
        "# Getting list with names of all layers from YOLO v3 network\r\n",
        "layers_names_all = network.getLayerNames()\r\n",
        "\r\n",
        "# # Check point\r\n",
        "# print()\r\n",
        "# print(layers_names_all)C:\\Users\\hafizurr\\Documents\\YOLO-3-OpenCV\\yolo-coco-data\\yolov4-obj_4000.weights\r\n",
        "\r\n",
        "# Getting only output layers' names that we need from YOLO v3 algorithm\r\n",
        "# with function that returns indexes of layers with unconnected outputs\r\n",
        "layers_names_output = \\\r\n",
        "    [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\r\n",
        "\r\n",
        "# # Check point\r\n",
        "# print()\r\n",
        "# print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\r\n",
        "\r\n",
        "# Setting minimum probability to eliminate weak predictions\r\n",
        "probability_minimum = 0.4\r\n",
        "\r\n",
        "# Setting threshold for filtering weak bounding boxes\r\n",
        "# with non-maximum suppression\r\n",
        "threshold = 0.4\r\n",
        "\r\n",
        "# Generating colours for representing every detected object\r\n",
        "# with function randint(low, high=None, size=None, dtype='l')\r\n",
        "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1P54mCp-3Ru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "693698dd-c5ab-429b-d2b4-b1affed61b4c"
      },
      "source": [
        "# start streaming video from webcam\r\n",
        "video_stream()\r\n",
        "# label for video\r\n",
        "label_html = 'Capturing...'\r\n",
        "# initialze bounding box to empty\r\n",
        "\r\n",
        "bbox = ''\r\n",
        "count = 0\r\n",
        "\r\n",
        "while True:\r\n",
        "    js_reply = video_frame(label_html, bbox)\r\n",
        "    if not js_reply:\r\n",
        "        break\r\n",
        "\r\n",
        "    # convert JS response to OpenCV Image\r\n",
        "    frame = js_to_image(js_reply[\"img\"])\r\n",
        "\r\n",
        "    h, w = frame.shape[:2]\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "    # create transparent overlay for bounding box\r\n",
        "    #bbox_array = np.zeros([480,640,4], dtype=np.uint8)\r\n",
        "\r\n",
        "    # call our darknet helper on video frame\r\n",
        "\r\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\r\n",
        "    network.setInput(blob)\r\n",
        "    output_from_network = network.forward(layers_names_output)\r\n",
        "\r\n",
        "    bounding_boxes = []\r\n",
        "    confidences = []\r\n",
        "    class_numbers = []\r\n",
        "\r\n",
        "    # Going through all output layers after feed forward pass\r\n",
        "    for result in output_from_network:\r\n",
        "        # Going through all detections from current output layer\r\n",
        "        for detected_objects in result:\r\n",
        "            # Getting 80 classes' probabilities for current detected object\r\n",
        "            scores = detected_objects[5:]\r\n",
        "            # Getting index of the class with the maximum value of probability\r\n",
        "            class_current = np.argmax(scores)\r\n",
        "            # Getting value of probability for defined class\r\n",
        "            confidence_current = scores[class_current]\r\n",
        "\r\n",
        "            # # Check point\r\n",
        "            # # Every 'detected_objects' numpy array has first 4 numbers with\r\n",
        "            # # bounding box coordinates and rest 80 with probabilities\r\n",
        "            # # for every class\r\n",
        "            # print(detected_objects.shape)  # (85,)\r\n",
        "\r\n",
        "            # Eliminating weak predictions with minimum probability\r\n",
        "            if confidence_current > probability_minimum:\r\n",
        "                # Scaling bounding box coordinates to the initial frame size\r\n",
        "                # YOLO data format keeps coordinates for center of bounding box\r\n",
        "                # and its current width and height\r\n",
        "                # That is why we can just multiply them elementwise\r\n",
        "                # to the width and height\r\n",
        "                # of the original frame and in this way get coordinates for center\r\n",
        "                # of bounding box, its width and height for original frame\r\n",
        "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\r\n",
        "\r\n",
        "                # Now, from YOLO data format, we can get top left corner coordinates\r\n",
        "                # that are x_min and y_min\r\n",
        "                x_center, y_center, box_width, box_height = box_current\r\n",
        "                x_min = int(x_center - (box_width / 2))\r\n",
        "                y_min = int(y_center - (box_height / 2))\r\n",
        "\r\n",
        "                # Adding results into prepared lists\r\n",
        "                bounding_boxes.append([x_min, y_min,\r\n",
        "                                       int(box_width), int(box_height)])\r\n",
        "                confidences.append(float(confidence_current))\r\n",
        "                class_numbers.append(class_current)\r\n",
        "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\r\n",
        "    if len(results) > 0:\r\n",
        "        # Going through indexes of results\r\n",
        "        for i in results.flatten():\r\n",
        "            # Getting current bounding box coordinates,\r\n",
        "            # its width and height\r\n",
        "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\r\n",
        "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\r\n",
        "\r\n",
        "            # Preparing colour for current bounding box\r\n",
        "            # and converting from numpy array to list\r\n",
        "            colour_box_current = colours[class_numbers[i]].tolist()\r\n",
        "\r\n",
        "            # # # Check point\r\n",
        "            # print(type(colour_box_current))  # <class 'list'>\r\n",
        "            # print(colour_box_current)  # [172 , 10, 127]\r\n",
        "\r\n",
        "            # Drawing bounding box on the original current frame\r\n",
        "            cv2.rectangle(frame, (x_min, y_min),\r\n",
        "                          (x_min + box_width, y_min + box_height),\r\n",
        "                          colour_box_current, 2)\r\n",
        "\r\n",
        "            # Preparing text with label and confidence for current bounding box\r\n",
        "            text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\r\n",
        "                                                   confidences[i])\r\n",
        "\r\n",
        "            # Putting text with label and confidence on the original image\r\n",
        "            cv2.putText(frame, text_box_current, (x_min, y_min - 5),\r\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\r\n",
        "    cv2.namedWindow('YOLO v3 Real Time Detections', cv2.WINDOW_NORMAL)\r\n",
        "    # Pay attention! 'cv2.imshow' takes images in BGR format\r\n",
        "    cv2.imshow('YOLO v3 Real Time Detections', frame)\r\n",
        "\r\n",
        "    # Breaking the loop if 'q' is pressed\r\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byrQnyE90xx-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFnUKMUJ0m5u"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}